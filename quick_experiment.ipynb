{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Quick Experiment: IPA vs English (phonemize → fine-tune IPA model → baseline BERT → compare)\n",
"\n",
"This Colab-friendly notebook runs a small, end-to-end experiment comparing a byte-robust model fine-tuned on **IPA**-transcribed text versus a baseline BERT fine-tuned on original English text.\n",
"\n",
"**Notes / tips before running:**\n",
"- This notebook uses small subsets (2k train / 1k test) so it runs quickly in Colab. Increase sizes if you have more GPU time.\n",
"- We use `espeak-ng` as backend for `phonemizer` (installed below).\n",
"- For a fairer comparison use identical train/val/test splits across conditions.\n",
"\n",
"Run cells top-to-bottom.\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Install system and python deps (Colab)\n",
"!apt-get -qq update && apt-get -qq install -y espeak-ng\n",
"!ln -sf /usr/bin/espeak-ng /usr/bin/espeak\n",
"!pip -q install transformers datasets phonemizer sentencepiece accelerate --upgrade\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Basic imports and GPU check\n",
"import os\n",
"import random\n",
"import numpy as np\n",
"import torch\n",
"from datasets import load_dataset, Dataset\n",
"from phonemizer import phonemize\n",
"from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding)\n",
"\n",
"print('torch:', torch.**version**)\n",
"print('cuda available:', torch.cuda.is_available())\n",
"print('device:', 'cuda' if torch.cuda.is_available() else 'cpu')\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Utility: cleaning and phonemizing functions\n",
"import re\n",
"def clean_texts(texts):\n",
"    # lowercase, normalize apostrophes, remove weird punctuation (keep letters, numbers, space, and apostrophe)\n",
"    cleaned = [t.lower().replace('’', "'") for t in texts]\n",
"    cleaned = [re.sub(r"[^a-z0-9' ]+", ' ', t) for t in cleaned]\n",
"    cleaned = [re.sub(r"\s+", ' ', t).strip() for t in cleaned]\n",
"    return cleaned\n",
"\n",
"def phonemize_batch(batch):\n",
"    # phonemize expects a list of strings; we use espeak backend\n",
"    return phonemize(batch, language='en-us', backend='espeak', strip=True, preserve_punctuation=False)\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Load IMDB (small subsets for quick runs)\n",
"raw = load_dataset('imdb')\n",
"\n",
"# fix random seed for reproducibility\n",
"seed = 42\n",
"random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
"\n",
"N_TRAIN = 2000  # change if you want larger experiments\n",
"N_TEST = 1000\n",
"\n",
"train_texts = raw['train']['text'][:N_TRAIN]\n",
"train_labels = raw['train']['label'][:N_TRAIN]\n",
"test_texts = raw['test']['text'][:N_TEST]\n",
"test_labels = raw['test']['label'][:N_TEST]\n",
"\n",
"# Clean texts\n",
"train_texts_clean = clean_texts(train_texts)\n",
"test_texts_clean = clean_texts(test_texts)\n",
"\n",
"print('Examples (clean):')\n",
"print(train_texts_clean[0][:200])\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Phonemize (IPA) the cleaned splits. Do in small batches to avoid timeouts.\n",
"BATCH = 200\n",
"ipa_train = []\n",
"for i in range(0, len(train_texts_clean), BATCH):\n",
"    batch = train_texts_clean[i:i+BATCH]\n",
"    ipa_batch = phonemize_batch(batch)\n",
"    ipa_train.extend(ipa_batch)\n",
"\n",
"ipa_test = []\n",
"for i in range(0, len(test_texts_clean), BATCH):\n",
"    batch = test_texts_clean[i:i+BATCH]\n",
"    ipa_batch = phonemize_batch(batch)\n",
"    ipa_test.extend(ipa_batch)\n",
"\n",
"print('Phonemized train examples:', len(ipa_train))\n",
"print(ipa_train[0][:200])\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Prepare HuggingFace datasets for Trainer API - IPA condition\n",
"ipa_train_ds = Dataset.from_dict({'text': ipa_train, 'label': train_labels})\n",
"ipa_test_ds = Dataset.from_dict({'text': ipa_test, 'label': test_labels})\n",
"\n",
"# We'll also keep the original English cleaned texts for baseline BERT\n",
"eng_train_ds = Dataset.from_dict({'text': train_texts_clean, 'label': train_labels})\n",
"eng_test_ds = Dataset.from_dict({'text': test_texts_clean, 'label': test_labels})\n",
"\n",
"print(ipa_train_ds, eng_train_ds)\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Tokenizer + model for IPA experiment (use a byte-robust model: roberta-base)\n",
"IPA_MODEL = 'roberta-base'\n",
"ipa_tokenizer = AutoTokenizer.from_pretrained(IPA_MODEL, use_fast=True)\n",
"ipa_model = AutoModelForSequenceClassification.from_pretrained(IPA_MODEL, num_labels=2)\n",
"\n",
"def tokenize_function(examples):\n",
"    return ipa_tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n",
"\n",
"ipa_train_tok = ipa_train_ds.map(tokenize_function, batched=True)\n",
"ipa_test_tok = ipa_test_ds.map(tokenize_function, batched=True)\n",
"\n",
"ipa_train_tok = ipa_train_tok.remove_columns(['text']).with_format('torch')\n",
"ipa_test_tok = ipa_test_tok.remove_columns(['text']).with_format('torch')\n",
"\n",
"print('IPA tokenized sample:', ipa_train_tok[0])\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Training arguments for IPA condition\n",
"training_args_ipa = TrainingArguments(\n",
"    output_dir='./results-ipa',\n",
"    evaluation_strategy='epoch',\n",
"    num_train_epochs=2,\n",
"    per_device_train_batch_size=8,\n",
"    per_device_eval_batch_size=32,\n",
"    logging_steps=50,\n",
"    save_strategy='no',\n",
"    report_to='none'\n",
")\n",
"\n",
"data_collator = DataCollatorWithPadding(tokenizer=ipa_tokenizer)\n",
"\n",
"from sklearn.metrics import accuracy_score, f1_score\n",
"def compute_metrics(pred):\n",
"    labels = pred.label_ids\n",
"    preds = pred.predictions.argmax(-1)\n",
"    return {'accuracy': accuracy_score(labels, preds), 'f1': f1_score(labels, preds)}\n",
"\n",
"trainer_ipa = Trainer(\n",
"    model=ipa_model,\n",
"    args=training_args_ipa,\n",
"    train_dataset=ipa_train_tok,\n",
"    eval_dataset=ipa_test_tok,\n",
"    tokenizer=ipa_tokenizer,\n",
"    data_collator=data_collator,\n",
"    compute_metrics=compute_metrics\n",
")\n",
"\n",
"# Train (IPA)\n",
"trainer_ipa.train()\n",
"res_ipa = trainer_ipa.evaluate()\n",
"print('IPA evaluation:', res_ipa)\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Baseline: BERT on original English cleaned texts\n",
"ENG_MODEL = 'bert-base-uncased'\n",
"eng_tokenizer = AutoTokenizer.from_pretrained(ENG_MODEL, use_fast=True)\n",
"eng_model = AutoModelForSequenceClassification.from_pretrained(ENG_MODEL, num_labels=2)\n",
"\n",
"def eng_tokenize(examples):\n",
"    return eng_tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n",
"\n",
"eng_train_tok = eng_train_ds.map(eng_tokenize, batched=True)\n",
"eng_test_tok = eng_test_ds.map(eng_tokenize, batched=True)\n",
"eng_train_tok = eng_train_tok.remove_columns(['text']).with_format('torch')\n",
"eng_test_tok = eng_test_tok.remove_columns(['text']).with_format('torch')\n",
"\n",
"training_args_eng = TrainingArguments(\n",
"    output_dir='./results-eng',\n",
"    evaluation_strategy='epoch',\n",
"    num_train_epochs=2,\n",
"    per_device_train_batch_size=8,\n",
"    per_device_eval_batch_size=32,\n",
"    logging_steps=50,\n",
"    save_strategy='no',\n",
"    report_to='none'\n",
")\n",
"trainer_eng = Trainer(\n",
"    model=eng_model,\n",
"    args=training_args_eng,\n",
"    train_dataset=eng_train_tok,\n",
"    eval_dataset=eng_test_tok,\n",
"    tokenizer=eng_tokenizer,\n",
"    data_collator=DataCollatorWithPadding(tokenizer=eng_tokenizer),\n",
"    compute_metrics=compute_metrics\n",
")\n",
"trainer_eng.train()\n",
"res_eng = trainer_eng.evaluate()\n",
"print('English BERT evaluation:', res_eng)\n"
]
},
{
"cell_type": "code",
"metadata": {},
"execution_count": null,
"outputs": [],
"source": [
"# Compare results\n",
"print('IPA model results:', res_ipa)\n",
"print('English BERT results:', res_eng)\n",
"\n",
"# Quick tokenization diagnostics\n",
"def token_stats(tokenizer, texts, name):\n",
"    toks = tokenizer(texts, truncation=True, padding=False)\n",
"    lengths = [len(x) for x in toks['input_ids']]\n",
"    print(f'{name} - mean tokens:', np.mean(lengths), 'median:', np.median(lengths), 'max:', np.max(lengths))\n",
"\n",
"token_stats(ipa_tokenizer, ipa_test[:200], 'IPA tokenization (sample)')\n",
"token_stats(eng_tokenizer, test_texts_clean[:200], 'English tokenization (sample)')\n"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"name": "python",
"version": "3.10"
}
},
"nbformat": 4,
"nbformat_minor": 5
}
